{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T06:28:21.244626Z",
     "start_time": "2020-06-09T06:28:21.240599Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import pandas as pd\n",
    "from nltk import RegexpTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T06:44:16.878215Z",
     "start_time": "2020-06-09T06:44:16.841734Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stopword_set = set(['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"])\n",
    "\n",
    "\n",
    "\"\"\"This function does all cleaning of data using two objects above\"\"\"\n",
    "def textCleaning(data):\n",
    "    new_str = data.lower()\n",
    "    dlist = tokenizer.tokenize(new_str)\n",
    "    dlist = list(set(dlist).difference(stopword_set))\n",
    "    return dlist[::-1]\n",
    "\n",
    "textCleaning(\"heLLO! World,\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traning D2V model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T06:44:04.013714Z",
     "start_time": "2020-06-09T06:43:48.299215Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['least',\n",
       " 'new',\n",
       " 'purse',\n",
       " 'spur',\n",
       " 'republicans',\n",
       " 'predicted',\n",
       " 'violates',\n",
       " 'evaluate',\n",
       " 'phillip',\n",
       " 'people',\n",
       " 'appealed',\n",
       " 'produce',\n",
       " 'government',\n",
       " 'receive',\n",
       " 'consequences',\n",
       " 'confidence',\n",
       " 'implode',\n",
       " 'time',\n",
       " 'white',\n",
       " 'win',\n",
       " 'vigorous',\n",
       " 'trump',\n",
       " 'destabilize',\n",
       " 'angering',\n",
       " 'despite',\n",
       " 'spokesman',\n",
       " 'coverage',\n",
       " 'questions',\n",
       " 'complicated',\n",
       " 'lack',\n",
       " 'stopped',\n",
       " 'congressional',\n",
       " 'speaker',\n",
       " 'era',\n",
       " 'place',\n",
       " 'behalf',\n",
       " 'years',\n",
       " 'market',\n",
       " 'litigation',\n",
       " 'election',\n",
       " 'broad',\n",
       " 'month',\n",
       " 'asserted',\n",
       " 'paying',\n",
       " 'reversed',\n",
       " 'reports',\n",
       " 'potential',\n",
       " 'reductions',\n",
       " 'decision',\n",
       " 'acknowledge',\n",
       " 'cascading',\n",
       " 'approval',\n",
       " 'drop',\n",
       " 'possibility',\n",
       " 'deductibles',\n",
       " 'chaos',\n",
       " 'found',\n",
       " 'widespread',\n",
       " 'proceedings',\n",
       " 'temporarily',\n",
       " 'lawyers',\n",
       " 'one',\n",
       " 'suit',\n",
       " 'disputed',\n",
       " 'whether',\n",
       " 'ugly',\n",
       " 'temporary',\n",
       " 'restore',\n",
       " 'congress',\n",
       " 'said',\n",
       " 'obama',\n",
       " 'inappropriate',\n",
       " 'settle',\n",
       " 'remained',\n",
       " 'defend',\n",
       " 'leverage',\n",
       " 'required',\n",
       " 'leadership',\n",
       " 'views',\n",
       " 'authority',\n",
       " 'stave',\n",
       " 'billion',\n",
       " 'states',\n",
       " 'victory',\n",
       " 'systems',\n",
       " 'intervene',\n",
       " 'fight',\n",
       " 'effects',\n",
       " 'limbo',\n",
       " 'set',\n",
       " 'worried',\n",
       " 'options',\n",
       " 'costs',\n",
       " 'ruled',\n",
       " 'continue',\n",
       " 'access',\n",
       " 'treasury',\n",
       " 'view',\n",
       " 'capitol',\n",
       " 'generally',\n",
       " 'inauguration',\n",
       " 'regarding',\n",
       " 'control',\n",
       " 'happens',\n",
       " 'divulge',\n",
       " 'awkward',\n",
       " 'program',\n",
       " 'jan',\n",
       " 'inclined',\n",
       " 'full',\n",
       " 'precedent',\n",
       " 'act',\n",
       " 'annual',\n",
       " 'executive',\n",
       " 'strategy',\n",
       " 'affordable',\n",
       " 'suspension',\n",
       " 'house',\n",
       " 'taking',\n",
       " 'prop',\n",
       " 'team',\n",
       " 'money',\n",
       " 'even',\n",
       " 'anticipated',\n",
       " 'lead',\n",
       " 'transition',\n",
       " 'future',\n",
       " 'may',\n",
       " 'another',\n",
       " 'judge',\n",
       " 'united',\n",
       " 'right',\n",
       " 'initially',\n",
       " 'passed',\n",
       " 'standing',\n",
       " 'demanding',\n",
       " 'put',\n",
       " 'well',\n",
       " 'given',\n",
       " 'yet',\n",
       " 'take',\n",
       " '2017',\n",
       " 'avoid',\n",
       " 'precedents',\n",
       " 'eligible',\n",
       " '13',\n",
       " 'important',\n",
       " '2010',\n",
       " 'seeking',\n",
       " 'washington',\n",
       " 'constitution',\n",
       " 'sudden',\n",
       " 'sue',\n",
       " 'payments',\n",
       " 'internal',\n",
       " 'desperate',\n",
       " 'mount',\n",
       " 'mr',\n",
       " 'shared',\n",
       " 'sums',\n",
       " 'preserving',\n",
       " 'flawed',\n",
       " 'insurance',\n",
       " 'ready',\n",
       " 'many',\n",
       " 'j',\n",
       " 'handle',\n",
       " 'appropriating',\n",
       " 'pending',\n",
       " 'hill',\n",
       " 'funding',\n",
       " 'republican',\n",
       " 'loss',\n",
       " 'dim',\n",
       " 'later',\n",
       " 'upon',\n",
       " 'nation',\n",
       " 'devastating',\n",
       " 'insurers',\n",
       " 'want',\n",
       " 'experts',\n",
       " 'overarching',\n",
       " 'consider',\n",
       " 'currently',\n",
       " 'ruling',\n",
       " 'find',\n",
       " 'quick',\n",
       " 'handing',\n",
       " 'effect',\n",
       " 'comment',\n",
       " 'matter',\n",
       " 'americans',\n",
       " 'successfully',\n",
       " 'totaled',\n",
       " 'prerogatives',\n",
       " 'dismiss',\n",
       " 'spending',\n",
       " 'request',\n",
       " 'aspects',\n",
       " 'though',\n",
       " 'championed',\n",
       " 'dispute',\n",
       " 'department',\n",
       " 'concepts',\n",
       " 'legal',\n",
       " 'columbia',\n",
       " 'would',\n",
       " 'permanent',\n",
       " 'race',\n",
       " 'repercussions',\n",
       " 'say',\n",
       " 'could',\n",
       " 'gaming',\n",
       " 'otherwise',\n",
       " 'committee',\n",
       " 'position',\n",
       " 'appeal',\n",
       " 'twist',\n",
       " 'law',\n",
       " 'health',\n",
       " 'contend',\n",
       " 'exchange',\n",
       " 'estimated',\n",
       " 'violation',\n",
       " 'related',\n",
       " 'february',\n",
       " 'prosecuting',\n",
       " 'political',\n",
       " 'lawsuit',\n",
       " 'dollars',\n",
       " 'took',\n",
       " 'two',\n",
       " 'huge',\n",
       " 'appropriation',\n",
       " 'conceivably',\n",
       " 'rosemary',\n",
       " 'district',\n",
       " 'subsidies',\n",
       " 'leaving',\n",
       " 'comes',\n",
       " 'officials',\n",
       " 'office',\n",
       " 'might',\n",
       " 'advocates',\n",
       " 'administration',\n",
       " 'power',\n",
       " 'care',\n",
       " 'conservative',\n",
       " 'court',\n",
       " 'quickly',\n",
       " 'voters',\n",
       " 'exit',\n",
       " 'told',\n",
       " 'leads',\n",
       " 'consumers',\n",
       " 'involves',\n",
       " 'longer',\n",
       " 'issue',\n",
       " 'appeals',\n",
       " 'outcome',\n",
       " 'late',\n",
       " 'prepared',\n",
       " 'pileup',\n",
       " 'discussing',\n",
       " 'proper',\n",
       " 'anticipating',\n",
       " 'sought',\n",
       " 'resolution',\n",
       " 'effort',\n",
       " 'incoming',\n",
       " 'replacement',\n",
       " 'blando',\n",
       " 'case',\n",
       " 'confident',\n",
       " 'backlash',\n",
       " 'entire',\n",
       " 'halt',\n",
       " '2015',\n",
       " 'fear',\n",
       " 'dynamics',\n",
       " 'eager',\n",
       " 'skepticism',\n",
       " 'donald',\n",
       " 'spend',\n",
       " 'seek',\n",
       " 'allies',\n",
       " 'gain',\n",
       " 'pressure',\n",
       " 'last',\n",
       " 'big',\n",
       " 'millions',\n",
       " 'losing',\n",
       " 'participants',\n",
       " 'never',\n",
       " 'provide',\n",
       " 'cause',\n",
       " 'since',\n",
       " 'branch',\n",
       " 'choose',\n",
       " 'resolve',\n",
       " 'without',\n",
       " 'part',\n",
       " 'appropriated',\n",
       " 'john',\n",
       " 'prevail',\n",
       " 'distributing',\n",
       " 'central',\n",
       " 'challenges',\n",
       " '20',\n",
       " 'justice',\n",
       " 'illustrating',\n",
       " 'issues',\n",
       " 'individuals',\n",
       " 'boehner',\n",
       " 'billions',\n",
       " 'circuit',\n",
       " 'presidential',\n",
       " 'suddenly',\n",
       " 'end',\n",
       " 'potentially',\n",
       " 'collyer',\n",
       " 'come',\n",
       " 'deal']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"get the text corpus from article source\"\"\"\n",
    "f = pd.read_csv('kaggle/articles1.csv')\n",
    "textCorpus = f[\"content\"]\n",
    "textCorpus = [textCleaning(text) for idx, text in textCorpus.iteritems()]\n",
    "textCorpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Tag documents\"\"\"\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(textCorpus)]\n",
    "\n",
    "\"\"\"Set model parameters\"\"\"\n",
    "max_epochs = 10\n",
    "vec_size   = 100\n",
    "alpha      = 0.025\n",
    "\n",
    "\n",
    "\"\"\"Initialise model\"\"\"\n",
    "textModel = Doc2Vec(vector_size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "\n",
    "\n",
    "\"\"\"Build vocab from tagged documents\"\"\"\n",
    "textModel.build_vocab(documents)\n",
    "\n",
    "\n",
    "\"\"\"Training D2V model\"\"\"\n",
    "for epoch in range(max_epochs):\n",
    "    print('iteration {0}'.format(epoch))\n",
    "    textModel.train(documents,\n",
    "                total_examples=textModel.corpus_count,\n",
    "                epochs=textModel.iter)\n",
    "    # decrease the learning rate\n",
    "    textModel.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    textModel.min_alpha = textModel.alpha\n",
    "\n",
    "\n",
    "textModel.save(\"d2v.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Vectorization with D2V Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T06:45:32.947565Z",
     "start_time": "2020-06-09T06:45:32.912405Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['article', 'sample']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"load pretrained model\"\"\"\n",
    "textModel = Doc2Vec.load(\"d2v.model\")\n",
    "\n",
    "\"\"\"Get and clean article content\"\"\"\n",
    "articleContent = \"This is a sample article\"\n",
    "articleContent = textCleaning(articleContent)\n",
    "articleContent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T06:45:44.265401Z",
     "start_time": "2020-06-09T06:45:44.260475Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.0245090e-03,  1.5786994e-03,  1.0835961e-03,  2.6223890e-04,\n",
       "       -1.6445892e-03,  1.5440167e-04, -4.2017670e-03,  3.5304213e-03,\n",
       "        2.0168030e-03,  3.8664429e-03, -2.5916628e-03, -1.7409977e-04,\n",
       "        2.5245168e-03, -4.8722062e-04, -3.7868381e-03, -2.0139995e-03,\n",
       "       -3.4781564e-03,  1.7303188e-03,  4.1704942e-04,  1.8745348e-03,\n",
       "        1.7303237e-03,  4.9541923e-03,  1.1248480e-03,  4.9768523e-03,\n",
       "       -2.1250462e-03,  4.8431065e-03, -3.2328602e-03, -4.9750758e-03,\n",
       "        4.4665211e-03, -3.0762405e-04,  2.7447208e-04, -2.2675802e-03,\n",
       "        3.4414111e-03, -2.4733644e-03,  1.1825904e-03,  1.6799474e-03,\n",
       "       -9.1594399e-04, -4.2598774e-03,  2.5454327e-03,  3.0918776e-03,\n",
       "       -2.5551862e-03, -3.4812214e-03,  2.8377303e-03,  3.6360780e-03,\n",
       "        1.2140333e-03, -2.5136117e-03, -1.1437719e-03, -4.5459303e-03,\n",
       "        3.5177174e-03,  4.1979630e-04, -2.5457493e-03,  3.0788423e-03,\n",
       "        9.0674097e-05, -3.0712376e-03,  2.1640278e-04, -4.5869546e-03,\n",
       "       -6.8223767e-04, -5.1768631e-04, -1.4002691e-03, -2.1396254e-03,\n",
       "        2.7194438e-03, -2.4535382e-05,  2.2709388e-03, -4.5531220e-03,\n",
       "       -3.8497651e-03,  1.6788457e-04,  6.2394497e-04, -3.2713057e-03,\n",
       "        1.3952135e-03,  2.4451804e-03,  1.7912450e-03,  4.2479523e-03,\n",
       "        6.5506541e-04,  1.2522495e-03,  2.0571526e-03, -3.4837825e-03,\n",
       "       -3.8309288e-03, -3.9633452e-03,  1.9564065e-03, -5.2888662e-04,\n",
       "        3.8645999e-04,  1.7060096e-03,  4.3807421e-03,  3.9909235e-03,\n",
       "       -1.3333537e-03, -3.2706873e-03,  3.4784852e-03, -1.4489314e-03,\n",
       "       -2.6952191e-03, -2.9994573e-03,  1.8671504e-03,  4.4000321e-03,\n",
       "        9.2008064e-04, -4.0391781e-03,  4.0998682e-03,  7.7353441e-04,\n",
       "       -1.7323027e-03,  3.3787037e-03,  1.5252269e-03, -8.3943189e-04],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = textModel.infer_vector(articleContent)\n",
    "vector"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
